<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Hand Gesture recognition | Uvais Karni</title> <meta name="author" content="Uvais Karni"/> <meta name="description" content="Developed an end to end image classification system using Fast R-CNN Network to predict 6 hand gestures."/> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://uvaiskarni.github.io/projects/3_project/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://uvaiskarni.github.io/"><span class="font-weight-bold">Uvais</span> Karni</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Curriculum Vitae</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Hand Gesture recognition</h1> <p class="post-description">Developed an end to end image classification system using Fast R-CNN Network to predict 6 hand gestures.</p> </header> <article> <p>The smart hand gesture recognition system can detect six different hand gestures, as listed in table below, and reacts accordingly. The system consists of three subsystems. The input of the system is a video containing a hand gesture, first subsystem processes the video and extract landmarks of the hand in each frame then pass it to the trained machine learning model to decide which hand gesture is present in each frame then the detected gesture will be shown to the user in order to give a voice command to Furhat robot with the detected gesture to do facial expressions and speech sentences depending upon the type of gesture.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/projects/Response_Gestures.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> System responses to gestures </div> <p>It has three subsystems: 1.Landmark extraction (First Sub-system), 2.Gesture recognition (Second Sub-system) and 3.Responsive behavior (Third Sub-system). Apart from these a fourth one called end to end subsystem (Specialisation).</p> <p>1.Landmark extraction (First Sub-system):</p> <p>This subsystem utilizes a pre-trained model from to detect the keypoints on different hand gesture images. The detector follows the architecture of Convolutional Pose Machines(CPMs). The subsystem 2 which is trained using the provided CSV data-set requires the landmark points to be structured as dorsal and palm points separately. To achieve this a separate SVM classifier is trained on a limited set of frames for each video to distinguish between the input frames as palm and dorsal regions. Then each video is read from the given data-set and the region type is predicted using the trained classifier. After that the pre-trained model[1] is loaded for keypoints detection and the landmark points are drawn. Based on the region type determined from the classifier the landmark points are structured to be passed to the subsystem 2 model to predict the final gesture type. To connect the subsystem 1 with subsystem 2 the model which was trained in the second subsystem using the CSV data-set is loaded and the gesture type is predicted by passing these landmark points obtained as input.</p> <p>2.Gesture recognition (Second Sub-system):</p> <p>Second Subsystem is developed using Keras, a deep learning API where the model is designed with three dense layers and relu activation function in the hidden layers and sigmoid function in the output layer. Stochastic gradient descent(SGD) is used as an optimizer function and categorical cross entropy as loss function. The model is trained for 10 epochs.</p> <p>3.Responsive behavior (Third Sub-system):</p> <p>The main functionality in the third subsystem is to react appropriately by Furhat social robot to the detected hand gesture that is detected by first and second subsystems. The initial design was to connect first and second subsystems to Furhat robot via TCP connection. Due to time limit, this feature is replaced by involving a human in the communication by reading the output of the second subsystem and give voice command to Furhat robot in order to react. Furhat robot reactions consists of two aspects: Facial expressions and saying a sentence. The input and output of this subsystem as shown in prior table above.</p> <p>4.End to End Subsystem (Specialisation):</p> <p>An Object Detection API to train a Fast Region Convolution Neural Network is used. The reason we chose Fast R-CNN is that it can classify multiple objects with low latency. The input features to the network are raw frames from the videos and output is a CSV file containing the desired class (Hand Gesture) and position of the hand in the frame. The Faster R-CNN has three components: – Convolution Layers: To extract features from images. – Region Proposal Network: To check whether the desired object is present or not. If the object is present then a bounding box for those object are given out. – Classes Detection: Here the bounded region part of the images are classified. It simply flattens the bounding region of the image and passes it through several dense layer and then a final Softmax layer which returns probabilities for each class.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/projects/H_01_res.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/projects/H_02_res.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0">&gt; <figure> <picture> <img src="/assets/img/projects/H_03_res.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Results from the end to end subsystems </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2022 Uvais Karni. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. Last updated: August 12, 2022. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>